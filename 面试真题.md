## 面试真题

### python中生成随机数，第一次[1,2,3]，以后仍然[1,2,3]，怎么做到？（赢时胜）

使用numpy

### **numpy.random.RandomState()**

伪随机数产生器的种子

对于某一个伪随机数发生器，只要该种子（seed）相同，产生的随机数序列就是相同的

### Python静态方法、类方法和实例方法区别？（赢时胜）

### Python内存泄露，如何避免，如何优化？（赢时胜）

有__del__() 函数的对象间的循环引用是导致内存泄漏的主凶。

不使用一个对象时使用:del object 来删除一个对象的引用计数就可以有效防止内存泄漏问题。

通过Python 扩展模块gc 来查看不能回收的对象的详细信息。

可以通过sys.getrefcount(obj) 来获取对象的引用计数，并根据返回值是否为0 来判断是否内存泄漏。

### Python 递归实现累加（赢时胜）

```python
def for_sum(n):
	"""
	累加求和
	1 + 2 + ... + n
	"""
    if n == 0:
        return 0
    return for_sum(n-1) + n
```

最快的就是数据方法求和了， (1+n)*n / 2

### MySQL索引（赢时胜）

TODO

### MySQL查询重复数据（赢时胜）

MYSQL查询重复记录的方法很多，下面就为您介绍几种最常用的[MYSQL](http://database.51cto.com/art/201011/235030.htm)查询重复记录的方法，希望对您学习MYSQL查询重复记录方面能有所帮助。

| rowId | peopleId |
| ----- | -------- |
|       |          |



1、查找表中多余的重复记录，重复记录是根据单个字段（peopleId）来判断

```sql
select * from people   
where peopleId in (select peopleId from people group by peopleId having count(peopleId) > 1)
```

2、删除表中多余的重复记录，重复记录是根据单个字段（peopleId）来判断，只留有rowid最小的记录

```sql
delete from people   
where peopleId in (select peopleId from people group by peopleId   having count(peopleId) > 1)   
and rowid not in (select min(rowid) from people group by peopleId having count(peopleId )>1)
```

3、查找表中多余的重复记录（多个字段）

```sql
select * from vitae a   
where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) > 1)   
```



4、删除表中多余的重复记录（多个字段），只留有rowid最小的记录

```sql
delete from vitae a   
where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) > 1)   
and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*)>1)   
```



5、查找表中多余的重复记录（多个字段），不包含rowid最小的记录

```sql
select * from vitae a   
where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) > 1)   
and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*)>1)   
```



### 100G文件，每行1个数字，如何快速计算出每个数字的出现次数？（赢时胜）

map/reduce， 分而治之，将大文件分成多个小文件，最后再将结合合并。类似于下题。

> 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 
>
> 　　方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。 
>
> 　　如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。 

### Nginx如何实现故障转移的，如何配置，原理呢？(第三石科技)

Nginx默认就是有故障转移的，只要在upstream里配置了多个host，默认的proxy_next_upstream

```
server  
{  
     listen       80;  
     server_name   www.yourdomain.com 192.168.203.42;  
     index index.html index.htm;  
     root   /data/htdocs/www;    
 
    location /  
    {  
     #故障转移的条件：如果后端的服务器返回502、504、执行超时等错误，自动将请求转发到upstream负载均衡池中的另一台服务器，实现故障转移。  
     proxy_next_upstream http_502 http_504 error timeout invalid_header;  
     
     proxy_send_timeout 10s;    # 代理发送超时时间
     proxy_read_timeout 10s;    # 代理接收超时时间
     proxy_next_upstream_tries 3;      # 重试次数

     proxy_cache cache_one;  
       
     #对不同的HTTP状态码设置不同的缓存时间  
     proxy_cache_valid   200 304 12h;  
       
     #以域名、URI、参数组合成Web缓存的Key值，Nginx根据Key值哈希，存储缓存内容到二级缓存目录内  
     proxy_cache_key $host$uri$is_args$args;  
     proxy_set_header Host   $host;  
     proxy_set_header X-Forwarded-For   $remote_addr;  
     proxy_pass http://backend_server;  
     expires       1d;  
    }  
} 

```



### 高并发的前提下，Session应当如何存储？(第三石科技)

### Celery了解吗？(第三石科技)

并发异步xx

### 用Redis实现限制爬虫，1秒最多只能请求10次，如何做？(第三石科技)

在Redis中set ip 请求次数 ex 1。

就是以请求的ip为键，值是请求次数，过期时间1秒。如果1秒超过了10次，那么就再创建一个"date"+ip 的键，值随意，过期时间24小时，表示这个ip 24小时之后才能再访问。